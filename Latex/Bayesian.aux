\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Probability, Entropy, and Inference}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Terminologt of inverse probability}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}An Example Inference Task: Clustering}{5}}
\@writefile{toc}{\contentsline {paragraph}{Reason 1}{5}}
\@writefile{toc}{\contentsline {paragraph}{Reason 2}{5}}
\@writefile{toc}{\contentsline {paragraph}{Reason 3}{5}}
\@writefile{toc}{\contentsline {paragraph}{Reason 4}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}K-means clustering}{5}}
\@writefile{toc}{\contentsline {paragraph}{Initialization.}{6}}
\@writefile{toc}{\contentsline {paragraph}{Assignment step.}{6}}
\newlabel{K}{{2.2}{6}}
\@writefile{toc}{\contentsline {paragraph}{Update step.}{6}}
\newlabel{update}{{2.4}{6}}
\@writefile{toc}{\contentsline {paragraph}{Repeat the assignment step and update step.}{6}}
\@writefile{toc}{\contentsline {paragraph}{\textbf  {\leavevmode {\color {red}Personal understanding of K-means algorithm}}}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Soft K-means clustering}{8}}
\@writefile{toc}{\contentsline {paragraph}{Assignment step.}{8}}
\@writefile{toc}{\contentsline {paragraph}{Updated step.}{8}}
\newlabel{update}{{2.7}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Exact Inference by Complete Enumeration}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Exact inference for continuous hypothesis spaces}{9}}
\@writefile{toc}{\contentsline {paragraph}{A two-parameter model}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Enumeration of an entire (discretized) hypothesis space for one Gaussian with parameters $\mu $ (horizontal axis) and $\delta $ (vertical).\relax }}{9}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:enumeration1}{{3.1}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Five datapoints.\relax }}{10}}
\newlabel{fig:datapoints1}{{3.2}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Likelihood function from figure 3.1\hbox {}, given the data of figure 3.2\hbox {}, represented by line thickness.\relax }}{10}}
\newlabel{fig:enumeration3}{{3.3}{10}}
\@writefile{toc}{\contentsline {paragraph}{A five-parameter mixture model}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Enumeration of the entire (discretized) hypothesis space for a mixture of two Gaussians. Weight of the mixture components is $\pi _1, \pi _2 = 0.6, 0.4$ in the top half and 0.8, 0.2 in the bottom half. Means $\mu _1 and \mu _2$ vary horizontally, and standard deviations $\delta _1 and \delta _2$ vary vertically.\relax }}{11}}
\newlabel{fig:enumeration4}{{3.4}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Inferring a mixture of two Gaussians. Likelihood function, given the data points, represented by line thikcness.\relax }}{11}}
\newlabel{fig:enumeration5}{{3.5}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Maximum Likelihood and Clustering}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Maximum likelihood for one Gaussian}{13}}
\newlabel{equation:maximum1}{{4.1}{13}}
\newlabel{equation:maximum2}{{4.2}{13}}
\newlabel{equation:maximum3}{{4.3}{13}}
\newlabel{equation:maximum4}{{4.4}{13}}
\newlabel{equation:align1}{{4.6}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Maximum likelihood for a mixture of Gaussians}{14}}
\newlabel{equation:mixtureofGaussian}{{4.7}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Enhancements to soft K-means}{15}}
\@writefile{toc}{\contentsline {paragraph}{Assignment step.}{15}}
\@writefile{toc}{\contentsline {paragraph}{Update step.}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}A fatal flaw of maximum likelihood}{16}}
\@writefile{toc}{\contentsline {paragraph}{Overfitting}{16}}
